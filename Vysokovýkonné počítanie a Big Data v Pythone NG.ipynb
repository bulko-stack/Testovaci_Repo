{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "Kni≈ænice HPC",
     "sheet_delimiter": true,
     "type": "MD"
    }
   },
   "source": [
    "# Kni≈ænice HPC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "gtjaH8gOiCJE2ArnONNN4I",
     "report_properties": {
      "rowId": "BuaxZHVowM1NDgIm5SGTW6"
     },
     "type": "MD"
    }
   },
   "source": [
    "## üêç Kurz Vysokov√Ωkonn√© poƒç√≠tanie a veƒæk√© d√°ta v Pythone (HPC)\n",
    "**üßë‚Äçüè´ Lektor:** Miroslav Reiter  \n",
    "**üì• LinkedIn kontakt:** https://www.linkedin.com/in/miroslav-reiter/  \n",
    "\n",
    "`*Testovaci* **Text**`\n",
    "\n",
    "**‚úÖ Osnova:** https://itkurzy.sav.sk/node/231\n",
    "\n",
    "**üéûÔ∏è YouTube vide√°:** https://www.youtube.com/c/IT-AcademySK  \n",
    "**üìá Zdrojov√© k√≥dy a materi√°ly:** https://github.com/miroslav-reiter/Kurzy_SAV_Analytika_Python_R  \n",
    "\n",
    "**üòä Emojis:** Win + .  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "HlJXCNvBPNinkUIhkHUYU9",
     "report_properties": {
      "rowId": "nT01gqoCPDuxVxR1lZEjaG"
     },
     "type": "MD"
    }
   },
   "source": [
    "# ‚öíÔ∏è Pou≈æ√≠van√© n√°stroje HPC\n",
    "- üêç **Python**: moment√°lne najpopul√°rnej≈°√≠ jazyk pre vedu o √∫dajoch (Data science). Python je s√≠ce interpretovan√Ω a pomal≈°√≠ jazyk ako C/C++, ale vyu≈æit√≠m kni≈æn√≠c, ako je NumPy (a Pandas navy≈°e), sa m√¥≈æeme pribl√≠≈æi≈• r√Ωchlosti C.  \n",
    "  \n",
    "- ü™ê **Jupyter**: n√°stroj s prostred√≠m interakt√≠vneho notebooku, mocn√Ω prieskumn√Ω, vizualizaƒçn√Ω a komunikaƒçn√Ω n√°stroj.\n",
    "  \n",
    "- üêº **Pandas**: n√°stroj na manipul√°ciu s tabuƒækov√Ωmi √∫dajmi v Pythone. Je to obrovsk√° kni≈ænica, ale ak pozn√°te spr√°vne met√≥dy, m√¥≈æeme vytvori≈• v√Ωkonn√∫ anal√Ωzu.\n",
    "\n",
    "- ü¶æ **Polars**: r√Ωchla kni≈ænica na spracovanie d√°t v Pythone, ktor√° je alternat√≠vou k Pandas. Polars je optimalizovan√° pre v√Ωkon, podporuje paraleln√© spracovanie d√°t a je schopn√° spracova≈• obrovsk√© mno≈æstv√° d√°t oveƒæa r√Ωchlej≈°ie ako Pandas.\n",
    "\n",
    "- üì¶ **Parquet**: Parquet je stƒ∫pcov√Ω form√°t na ukladanie d√°t, ktor√Ω je optimalizovan√Ω pre ƒç√≠tanie a z√°pis veƒæk√Ωch datasetov. Je be≈æne pou≈æ√≠van√Ω v big data aplik√°ci√°ch a podporuje efekt√≠vne kompresie a r√Ωchle vyhƒæad√°vanie.\n",
    "\n",
    "- üî• **Apache Spark**: open-source distribuovan√° v√Ωpoƒçtov√° platforma, ktor√° umo≈æ≈àuje paraleln√© spracovanie veƒæk√Ωch objemov d√°t. Je ≈°iroko pou≈æ√≠van√° na anal√Ωzu big data, strojov√© uƒçenie a spracovanie d√°t v cloude. Spark poskytuje podporu pre spracovanie d√°t v re√°lnom ƒçase a je kompatibiln√Ω s form√°tmi ako Parquet a CSV.\n",
    "  \n",
    "- üìä **NumPy**: pre numerick√© v√Ωpoƒçty v Pythone, ktor√° poskytuje efekt√≠vnu pr√°cu s viacrozmern√Ωmi poƒæami a maticami. Pou≈æ√≠va sa na r√Ωchle numerick√© oper√°cie, line√°rnu algebru a spracovanie veƒæk√Ωch d√°t.\n",
    "\n",
    "- üî¨ **SciPy**: postaven√° na NumPy a prid√°va pokroƒçil√© matematick√© funkcie ako numerick√° integr√°cia, optimaliz√°cia a ≈°tatistika. Je ide√°lna pre vedeck√© a in≈æinierske v√Ωpoƒçty.\n",
    "\n",
    "- üñß **Dask**: paraleln√© a distribuovan√© v√Ωpoƒçty na veƒæk√Ωch d√°tach cez viacer√© procesy a uzly. Je kompatibiln√Ω s Pandas a NumPy, ƒç√≠m zjednodu≈°uje pr√°cu s veƒæk√Ωmi datasetmi v distribuovan√Ωch prostrediach.\n",
    "\n",
    "- ü§ñ **PyTorch**: pre strojov√© uƒçenie, ktor√° podporuje v√Ωpoƒçty na GPU. Je obƒæ√∫ben√° pre dynamick√© modelovanie a experimentovanie, hlavne v oblasti poƒç√≠taƒçov√©ho videnia a NLP.\n",
    "\n",
    "- üìà **TensorFlow**: na vytv√°ranie a nasadzovanie modelov strojov√©ho uƒçenia v produkƒçn√Ωch prostrediach. Je optimalizovan√° na vysok√Ω v√Ωkon, distrib√∫ciu a spracovanie veƒæk√Ωch d√°t.\n",
    "\n",
    "- üîó **MPI for Python (mpi4py)**: na implement√°ciu paraleln√Ωch algoritmov cez MPI (Message Passing Interface), urƒçen√° pre distribuovan√© a paraleln√© v√Ωpoƒçty v prostrediach ako superpoƒç√≠taƒçe a klastry.\n",
    "\n",
    "- ‚ö° **CUDA**: platforma a API od NVIDIA na paraleln√© v√Ωpoƒçty pomocou GPU. PyCUDA umo≈æ≈àuje vyu≈æ√≠va≈• GPU pre vysokov√Ωkonn√© v√Ωpoƒçty, ƒço ur√Ωchƒæuje aplik√°cie ako simul√°cie, spracovanie obrazu a strojov√© uƒçenie.\n",
    "\n",
    "üòé Existuje 300 000+ kni≈æn√≠c pre Python.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "utjXvhnaVEZx0y9tcUL1AW",
     "report_properties": {
      "rowId": "HnEJbqSEA5YeuN0S9J1MeG"
     },
     "type": "MD"
    }
   },
   "source": [
    "# üìö PIP, Kni≈ænice a ich verzie \n",
    "**PIP (package installer for Python)** - Spr√°vca Bal√≠ƒçkov pre moduly programovacieho jazyka Python, Tie≈æ nap√≠san√Ω v Pythone, Licencia MIT.  \n",
    "Dokument√°cia PIP: https://pip.pypa.io/en/stable/\n",
    "\n",
    "**PyPI (The Python Package Index)** - Repozit√°r/√∫lo≈æisko softv√©ru pre programovac√≠ jazyk Python  \n",
    "https://pypi.org\n",
    "1\n",
    "\n",
    "!pip list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "Qf2fGBT29Tp85ig2h3H6be",
     "report_properties": {
      "rowId": "iF77wHN3KjirO3UBNMvq29"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package           Version\n",
      "----------------- -----------\n",
      "absl-py           2.1.0\n",
      "asttokens         2.4.1\n",
      "bokeh             3.6.1\n",
      "click             8.1.7\n",
      "cloudpickle       3.1.0\n",
      "colorama          0.4.6\n",
      "comm              0.2.2\n",
      "contourpy         1.3.1\n",
      "cycler            0.12.1\n",
      "Cython            3.0.11\n",
      "dask              2024.11.2\n",
      "dask-expr         1.1.19\n",
      "debugpy           1.8.8\n",
      "decorator         5.1.1\n",
      "distributed       2024.11.2\n",
      "executing         2.1.0\n",
      "fonttools         4.54.1\n",
      "fsspec            2024.10.0\n",
      "h5py              3.12.1\n",
      "ipykernel         6.29.5\n",
      "ipython           8.29.0\n",
      "jedi              0.19.2\n",
      "Jinja2            3.1.4\n",
      "joblib            1.4.2\n",
      "jupyter_client    8.6.3\n",
      "jupyter_core      5.7.2\n",
      "keras             3.6.0\n",
      "kiwisolver        1.4.7\n",
      "locket            1.0.0\n",
      "lz4               4.3.3\n",
      "markdown-it-py    3.0.0\n",
      "MarkupSafe        3.0.2\n",
      "matplotlib        3.9.2\n",
      "matplotlib-inline 0.1.7\n",
      "mdurl             0.1.2\n",
      "ml_dtypes         0.5.0\n",
      "mpi4py            4.0.1\n",
      "msgpack           1.1.0\n",
      "namex             0.0.8\n",
      "nest-asyncio      1.6.0\n",
      "numpy             2.1.3\n",
      "optree            0.13.1\n",
      "packaging         24.2\n",
      "pandas            2.2.3\n",
      "parquet           1.3.1\n",
      "parso             0.8.4\n",
      "partd             1.4.2\n",
      "pillow            11.0.0\n",
      "pip               24.3.1\n",
      "platformdirs      4.3.6\n",
      "plotly            5.24.1\n",
      "ply               3.11\n",
      "polars            1.13.0\n",
      "prompt_toolkit    3.0.48\n",
      "psutil            6.1.0\n",
      "pure_eval         0.2.3\n",
      "py4j              0.10.9.7\n",
      "pyarrow           18.0.0\n",
      "Pygments          2.18.0\n",
      "pyparsing         3.2.0\n",
      "pyspark           3.5.3\n",
      "python-dateutil   2.9.0.post0\n",
      "pytube            15.0.0\n",
      "pytz              2024.2\n",
      "pywin32           308\n",
      "PyYAML            6.0.2\n",
      "pyzmq             26.2.0\n",
      "rich              13.9.4\n",
      "scikit-learn      1.5.2\n",
      "scipy             1.14.1\n",
      "seaborn           0.13.2\n",
      "six               1.16.0\n",
      "sortedcontainers  2.4.0\n",
      "squarify          0.4.4\n",
      "stack-data        0.6.3\n",
      "tblib             3.0.0\n",
      "tenacity          9.0.0\n",
      "threadpoolctl     3.5.0\n",
      "thriftpy2         0.5.2\n",
      "toolz             1.0.0\n",
      "tornado           6.4.1\n",
      "traitlets         5.14.3\n",
      "typing_extensions 4.12.2\n",
      "tzdata            2024.2\n",
      "urllib3           2.2.3\n",
      "wcwidth           0.2.13\n",
      "xyzservices       2024.9.0\n",
      "zict              3.0.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: parquet in c:\\users\\bulko\\appdata\\roaming\\python\\python313\\site-packages (1.3.1)\n",
      "Requirement already satisfied: thriftpy2 in c:\\users\\bulko\\appdata\\roaming\\python\\python313\\site-packages (from parquet) (0.5.2)\n",
      "Requirement already satisfied: Cython>=3.0.10 in c:\\users\\bulko\\appdata\\roaming\\python\\python313\\site-packages (from thriftpy2->parquet) (3.0.11)\n",
      "Requirement already satisfied: ply<4.0,>=3.4 in c:\\users\\bulko\\appdata\\roaming\\python\\python313\\site-packages (from thriftpy2->parquet) (3.11)\n",
      "Requirement already satisfied: six~=1.15 in c:\\users\\bulko\\appdata\\roaming\\python\\python313\\site-packages (from thriftpy2->parquet) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip list\n",
    "\n",
    "#!pip install numpy\n",
    "#!pip install scipy\n",
    "#!pip install dask[complete]\n",
    "#!pip install torch\n",
    "#!pip install tensorflow\n",
    "#!pip install polars\n",
    "!pip install parquet\n",
    "#!pip install pyspark\n",
    "#!pip install pyarrow\n",
    "#!pip install mpi4py\n",
    "#!pip install pycuda"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "O6f5kWGeujRXtOQVWAlLTl",
     "report_properties": {
      "rowId": "E9cX2hSUAeBjar0TwM5syB"
     },
     "type": "MD"
    }
   },
   "source": [
    "# üìú Verzie vizualizaƒçn√Ωch kni≈ænic \n",
    "try - except - finally  \n",
    "\n",
    "https://docs.python.org/3/tutorial/errors.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "V0v8HOOhd9hhVxJnX8q4eL",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Moduly/kniznice boli nacitane\n",
      "Pandas verzia 2.2.3\n",
      "Numpy verzia 2.1.3\n",
      "Scipy verzia 1.14.1\n",
      "Dask verzia 2024.11.2\n",
      "Polars verzia 1.13.0\n",
      "PyArrow verzia 18.0.0\n",
      "Parquet verzia 1.3.1\n",
      "Pyspark verzia 3.5.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import scipy \n",
    "import dask\n",
    "#import torch\n",
    "#import tensorflow as tf\n",
    "\n",
    "import pyspark\n",
    "# na pracu s parquet format\n",
    "import pyarrow\n",
    "import parquet\n",
    "\n",
    "import importlib.metadata\n",
    "\n",
    "print(\"‚úÖ Moduly/kniznice boli nacitane\")\n",
    "# Zobrazenie verzi√≠ kni≈æn√≠c\n",
    "print(f\"Pandas verzia {pd.__version__}\")\n",
    "print(f\"Numpy verzia {np.__version__}\")\n",
    "print(f\"Scipy verzia {scipy.__version__}\")\n",
    "print(f\"Dask verzia {dask.__version__}\")\n",
    "#print(f\"Pytorch verzia {torch.__version__}\")\n",
    "#print(f\"Tensorflow verzia {tf.__version__}\")\n",
    "\n",
    "print(f\"Polars verzia {pl.__version__}\")\n",
    "\n",
    "print(f\"PyArrow verzia {pyarrow.__version__}\")\n",
    "print(f\"Parquet verzia {importlib.metadata.version('parquet')}\")\n",
    "\n",
    "print(f\"Pyspark verzia {pyspark.__version__}\")\n",
    "\n",
    "\n",
    "\n",
    "# Tu pou≈æ√≠vame pyarrow pre Parquet\n",
    " \n",
    "\n",
    "# Ak by bol nain≈°talovan√Ω `pyarrow` pre Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "xxC61eI2Oqyzy8GmzzwrET",
     "report_properties": {
      "rowId": "csTaUBzwY1G6f6IoNz36lu"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Moduly/kniznice boli nacitane\n",
      "Pandas verzia 2.2.3\n",
      "Numpy verzia 2.1.3\n",
      "Scipy verzia 1.14.1\n",
      "Dask verzia 2024.11.2\n",
      "Polars verzia 1.13.0\n",
      "PyArrow verzia 18.0.0\n",
      "Parquet verzia 1.3.1\n",
      "Pyspark verzia 3.5.3\n",
      "\n",
      "FINALLY: Tento text sa vypise vzdy\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import pandas as pd\n",
    "    import polars as pl\n",
    "    import numpy as np\n",
    "    import scipy \n",
    "    import dask\n",
    "    #import torch\n",
    "    #import tensorflow as tf\n",
    "\n",
    "    import pyspark\n",
    "# na pracu s parquet format\n",
    "    import pyarrow\n",
    "    import parquet\n",
    "\n",
    "    import importlib.metadata\n",
    "\n",
    "    print(\"‚úÖ Moduly/kniznice boli nacitane\")\n",
    "# Zobrazenie verzi√≠ kni≈æn√≠c\n",
    "    print(f\"Pandas verzia {pd.__version__}\")\n",
    "    print(f\"Numpy verzia {np.__version__}\")\n",
    "    print(f\"Scipy verzia {scipy.__version__}\")\n",
    "    print(f\"Dask verzia {dask.__version__}\")\n",
    "    #print(f\"Pytorch verzia {torch.__version__}\")\n",
    "    #print(f\"Tensorflow verzia {tf.__version__}\")\n",
    "\n",
    "    print(f\"Polars verzia {pl.__version__}\")\n",
    "\n",
    "    print(f\"PyArrow verzia {pyarrow.__version__}\")\n",
    "    print(f\"Parquet verzia {importlib.metadata.version('parquet')}\")\n",
    "\n",
    "    print(f\"Pyspark verzia {pyspark.__version__}\")\n",
    "\n",
    "\n",
    "\n",
    "# Tu pou≈æ√≠vame pyarrow pre Parquet\n",
    " \n",
    "\n",
    "# Ak by bol nain≈°talovan√Ω `pyarrow` pre Parquet\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "except:\n",
    "    print(\"EXCEPT: Moduly/Kniznice neboli nacitane, nastala chyba...\")\n",
    "\n",
    "finally:\n",
    "    print(\"\\nFINALLY: Tento text sa vypise vzdy\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "9nZeU67HPQXJSgN5qZb7h6",
     "type": "MD"
    }
   },
   "source": [
    "# ü§ñ TensorFlow: Z√°kladn√Ω model neur√≥novej siete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "ZcvVIYsnByU77rGQw9mYQw",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential([\n\u001b[0;32m      6\u001b[0m     Dense(\u001b[38;5;241m32\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m784\u001b[39m,)),\n\u001b[0;32m      7\u001b[0m     Dense(\u001b[38;5;241m10\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m ])\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, activation=\"relu\", input_shape=(784,)),\n",
    "    Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer = \"adam\",\n",
    "    loss = \"sparse_categorical_crossentropy\",\n",
    "    metrics = [\"accuracy\"]\n",
    ")\n",
    "\n",
    "X_train = np.random.random((1000, 784))\n",
    "Y_train = np.random.randint(10, size = (1000, ))\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    epochs = 5\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "tsdIQ9XGjereK4lpOTMNED",
     "type": "MD"
    }
   },
   "source": [
    "# ‚ö° Polars: R√Ωchle spracovanie d√°tov√Ωch r√°mcov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "lxu23ZZDtd75p5JMDyXnt8",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (4, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>A</th><th>B</th><th>C</th></tr><tr><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>1</td><td>5</td><td>1</td></tr><tr><td>2</td><td>6</td><td>2</td></tr><tr><td>3</td><td>7</td><td>3</td></tr><tr><td>4</td><td>8</td><td>4</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (4, 3)\n",
       "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
       "‚îÇ A   ‚îÜ B   ‚îÜ C   ‚îÇ\n",
       "‚îÇ --- ‚îÜ --- ‚îÜ --- ‚îÇ\n",
       "‚îÇ i64 ‚îÜ i64 ‚îÜ i64 ‚îÇ\n",
       "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
       "‚îÇ 1   ‚îÜ 5   ‚îÜ 1   ‚îÇ\n",
       "‚îÇ 2   ‚îÜ 6   ‚îÜ 2   ‚îÇ\n",
       "‚îÇ 3   ‚îÜ 7   ‚îÜ 3   ‚îÇ\n",
       "‚îÇ 4   ‚îÜ 8   ‚îÜ 4   ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "df = pl.DataFrame({\n",
    "    \"A\": [1,2,3,4],\n",
    "    \"B\": [5,6,7,8]\n",
    "\n",
    "}\n",
    ")\n",
    "\n",
    "#print(df)\n",
    "#df\n",
    "\n",
    "df_filtrovany = df.filter(pl.col(\"A\")>2)\n",
    "#df_filtrovany\n",
    "\n",
    "df2 = df.with_columns(pl.col(\"A\").alias(\"C\"))\n",
    "df2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "B6sjX6sDMFTUWo9sNL2dyT",
     "type": "MD"
    }
   },
   "source": [
    "# üì¶ PyArrow: Pr√°ca s Parquet form√°tom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "Go4cn1W4BSpOvJIdPZK4vf",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "meno: string\n",
       "vek: int32\n",
       "povolania: string\n",
       "----\n",
       "meno: [[\"Dada\",\"Martin\",\"Michal\"]]\n",
       "vek: [[21,28,29]]\n",
       "povolania: [[\"In≈æinier\",\"Doktor\",\"Umelec\"]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq \n",
    "import pandas as pd\n",
    "\n",
    "data = [\n",
    "    (\"Dada\", 21, \"In≈æinier\"),\n",
    "    (\"Martin\", 28, \"Doktor\"),\n",
    "    (\"Michal\", 29, \"Umelec\")\n",
    "]\n",
    "\n",
    "schema = pa.schema([\n",
    "    (\"meno\", pa.string()),\n",
    "    (\"vek\", pa.int32()),\n",
    "    (\"povolania\", pa.string())\n",
    "])\n",
    "\n",
    "tabulka = pa.Table.from_pandas(pd.DataFrame(data, columns=[\"meno\", \"vek\", \"povolania\"]), schema = schema)\n",
    "\n",
    "pq.write_table(tabulka, \"zamestnanci.parquet\")\n",
    "\n",
    "\n",
    "tabulka_nacitana = pq.read_table(\"zamestnanci.parquet\")\n",
    "tabulka_nacitana"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "H6NTYiRAnYfsGkZA9mtMGr",
     "type": "MD"
    }
   },
   "source": [
    "# üî• PySpark: Pr√°ca s distribuovan√Ωmi d√°tami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "Zad9yKX9TuNDrxSMn2AyFX",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.UnsupportedOperationException: getSubject is supported only if a security manager is allowed\r\n\tat java.base/javax.security.auth.Subject.getSubject(Subject.java:347)\r\n\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:577)\r\n\tat org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2416)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2416)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:329)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:501)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:485)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1575)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m----> 3\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPrikladPySpark\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m data \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      6\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRobert\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m40\u001b[39m),\n\u001b[0;32m      7\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJan\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m30\u001b[39m),\n\u001b[0;32m      8\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMartin\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m      9\u001b[0m ]\n\u001b[0;32m     11\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(data, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeno\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvek\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pyspark\\sql\\session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pyspark\\context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pyspark\\context.py:203\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    201\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mudf_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pyspark\\context.py:296\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    295\u001b[0m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[1;32m--> 296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;241m=\u001b[39m jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m SparkConf(_jconf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39mconf())\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pyspark\\context.py:421\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[1;34m(self, jconf)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;124;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\py4j\\java_gateway.py:1587\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1581\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1582\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1583\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1584\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1586\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1587\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1588\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1590\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1591\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.UnsupportedOperationException: getSubject is supported only if a security manager is allowed\r\n\tat java.base/javax.security.auth.Subject.getSubject(Subject.java:347)\r\n\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:577)\r\n\tat org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2416)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2416)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:329)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:501)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:485)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1575)\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PrikladPySpark\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (\"Robert\", 40),\n",
    "    (\"Jan\", 30),\n",
    "    (\"Martin\", 20)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"meno\", \"vek\"])\n",
    "df.show()\n",
    "\n",
    "df_filtrovany = df.filter(df[\"vek\"] >= 30)\n",
    "df_filtrovany.show()\n",
    "\n",
    "df.write.mode(\"ignore\").parquet(\"studenti.parquet\")\n",
    "\n",
    "df_nacitany = spark.read.parquet(\"studenti.parquet\")\n",
    "df_nacitany.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "Testovacie D√°ta a Generovanie D√°t",
     "sheet_delimiter": true,
     "type": "MD"
    }
   },
   "source": [
    "# Testovacie D√°ta a Generovanie D√°t"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "HcTDYNQOTjjFJgdsyA5vLI",
     "type": "MD"
    }
   },
   "source": [
    "# Testovacie D√°ta a Generovanie D√°t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "iFZ3t2RO2PxxNPuUGpVNXI",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "zaznamy = 8_000_000\n",
    "\n",
    "data_klienti = {\n",
    "    \"Meno\": np.random.choice([\n",
    "        \"Adriana\", \"Alena\", \"Andrea\", \"Anna\", \"Barbora\", \"Be√°ta\", \"Daniela\", \"Dominika\", \"Elena\", \"Em√≠lia\", \"Erika\", \"Eva\", \"Gabriela\", \"Hana\", \"Ivana\", \"Jana\", \"Katar√≠na\", \"Krist√≠na\", \n",
    "    \"Laura\", \"Lenka\", \"Lucia\", \"Magdal√©na\", \"M√°ria\", \"Martina\", \"Monika\", \"Nat√°lia\", \"Petra\", \"Ren√°ta\", \"Silvia\", \"Simona\", \"So≈àa\", \"Tamara\", \"Veronika\", \"Zuzana\"\n",
    "    ], size = zaznamy),\n",
    "    \"Vek\": np.random.randint(18, 71, size = zaznamy),\n",
    "    \"Mesta\": np.random.choice([\"Bratislava\", \"Ko≈°ice\", \"Pre≈°ov\", \"≈Ωilina\", \"Nitra\", \"Bansk√° Bystrica\", \"Trnava\", \"Martin\", \"Trenƒç√≠n\", \"Poprad\"], size = zaznamy),\n",
    "    \"Nakupy\": np.round(np.random.uniform(150.0, 1000.0, size = zaznamy), 2)\n",
    "}\n",
    "\n",
    "df_data_klienti = pd.DataFrame(data_klienti)\n",
    "df_data_klienti\n",
    "df_data_klienti.to_csv(\"dataset-8M-klient-sk.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "IUSdvy3AyzABVb5YhVEQZ7",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Poƒçet z√°znamov\n",
    "zaznamy = 1_000_000_000\n",
    "# Veƒækos≈• jednej ƒçasti\n",
    "velkost_casti = 10_000_000\n",
    "\n",
    "# Cesta k s√∫boru\n",
    "csv_soubor = \"dataset-1000M-klienti-SK.csv\"\n",
    "\n",
    "# Nastav√≠me, aby sa s√∫bor prepisoval od zaƒçiatku, keƒè≈æe budeme zapisova≈• po ƒçastiach\n",
    "with open(csv_soubor, mode=\"a\") as f:\n",
    "    # Vytvorenie d√°t a z√°pis po ƒçastiach\n",
    "    for i in range(zaznamy // velkost_casti):\n",
    "        data_klienti = {\n",
    "            \"Meno\": np.random.choice([\n",
    "                \"Adriana\", \"Alena\", \"Andrea\", \"Anna\", \"Barbora\", \"Be√°ta\", \"Daniela\", \"Dominika\", \n",
    "                \"Elena\", \"Em√≠lia\", \"Erika\", \"Eva\", \"Gabriela\", \"Hana\", \"Ivana\", \"Jana\", \"Katar√≠na\", \n",
    "                \"Krist√≠na\", \"Laura\", \"Lenka\", \"Lucia\", \"Magdal√©na\", \"M√°ria\", \"Martina\", \"Monika\", \n",
    "                \"Nat√°lia\", \"Petra\", \"Ren√°ta\", \"Silvia\", \"Simona\", \"So≈àa\", \"Tamara\", \"Veronika\", \"Zuzana\"], \n",
    "                size=velkost_casti\n",
    "            ),\n",
    "            \"Vek\": np.random.randint(18, 70, size=velkost_casti),\n",
    "            \"Mesto\": np.random.choice([\n",
    "                \"Bratislava\", \"Ko≈°ice\", \"Pre≈°ov\", \"≈Ωilina\", \"Nitra\", \n",
    "                \"Bansk√° Bystrica\", \"Trnava\", \"Martin\", \"Trenƒç√≠n\", \"Poprad\"\n",
    "            ], size=velkost_casti),\n",
    "            \"Nakupy\": np.round(np.random.uniform(150.0, 1000.0, size=velkost_casti), 2)\n",
    "        }\n",
    "        \n",
    "        # Vytvorenie DataFrame pre aktu√°lnu ƒças≈•\n",
    "        df_data_klienti = pd.DataFrame(data_klienti)\n",
    "        \n",
    "        # Prv√Ωkr√°t zapisujeme hlaviƒçku, ostatn√© ƒçasti u≈æ nie\n",
    "        df_data_klienti.to_csv(csv_soubor, mode=\"a\", header=(i == 0), index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "jlFOtTvW4oV7pW67aR0SQe",
     "type": "MD"
    }
   },
   "source": [
    "# üìö Kni≈ænica Faker\n",
    "- Popul√°rna kni≈ænica v Pythone, ktor√° sl√∫≈æi na generovanie falo≈°n√Ωch √∫dajov, ako s√∫ men√°, adresy, e-maily, telef√≥nne ƒç√≠sla a ƒèal≈°ie. \n",
    "- Je veƒæmi u≈æitoƒçn√° pri testovan√≠ aplik√°ci√≠, generovan√≠ testovac√≠ch d√°t alebo pri simul√°cii re√°lnych pou≈æ√≠vateƒæov. \n",
    "- M√¥≈æete r√Ωchlo a jednoducho vytv√°ra≈• realisticky vyzeraj√∫ce, ale √∫plne n√°hodn√© √∫daje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "YkTIwPISvzAI8KhHgAYDSC",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faker\r\n",
      "  Downloading Faker-30.8.2-py3-none-any.whl.metadata (15 kB)\r\n",
      "Requirement already satisfied: python-dateutil>=2.4 in /opt/python/envs/default/lib/python3.8/site-packages (from faker) (2.9.0.post0)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/python/envs/default/lib/python3.8/site-packages (from faker) (4.5.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/python/envs/default/lib/python3.8/site-packages (from python-dateutil>=2.4->faker) (1.16.0)\r\n",
      "Downloading Faker-30.8.2-py3-none-any.whl (1.8 MB)\r\n",
      "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m106.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: faker\r\n",
      "Successfully installed faker-30.8.2\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "B67OvR2oyWzIqMQH5N0XJt",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. pou≈æ√≠vateƒæ : V√°clav Mr√°z\n",
      "Email: v√°clav.mr√°z1@gmail.com\n",
      "Adresa: Nov√° Bellova 3\n",
      "804 04 Licince\n",
      "Telef√≥n: 00421 948 582 742\n",
      "----------------------------------------\n",
      "2. pou≈æ√≠vateƒæ : Kazim√≠r Klimkov√°\n",
      "Email: kazim√≠r.klimkov√°2@gmail.com\n",
      "Adresa: Mal√Ω trh 66\n",
      "828 54 Nevidzany\n",
      "Telef√≥n: 00421 948 545 248\n",
      "----------------------------------------\n",
      "3. pou≈æ√≠vateƒæ : Lolita Dorov√°\n",
      "Email: lolita.dorov√°3@gmail.com\n",
      "Adresa: ≈†pieszova 7\n",
      "960 45 Mal√© Kr≈°te≈àany\n",
      "Telef√≥n: 00421 58 612 2829\n",
      "----------------------------------------\n",
      "4. pou≈æ√≠vateƒæ : Miriama Jendekov√°\n",
      "Email: miriama.jendekov√°4@gmail.com\n",
      "Adresa: √ödoln√° 82\n",
      "996 85 Skalka nad V√°hom\n",
      "Telef√≥n: +421 949 665 893\n",
      "----------------------------------------\n",
      "5. pou≈æ√≠vateƒæ : Linda Pavelkov√°\n",
      "Email: linda.pavelkov√°5@gmail.com\n",
      "Adresa: Pasienkov√° 3\n",
      "042 23 Stanƒça\n",
      "Telef√≥n: 00421 901 245 494\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "vrn96tXTQ6yqoIhkl19ZqK",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "generuj_obrazok() takes 0 positional arguments but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------",
      "Traceback (most recent call last)",
      "generuj_obrazok() takes 0 positional arguments but 2 were given"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "p4tLTia16lRqDEUC3Bjvq7",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL falo≈°n√©ho obr√°zka: https://picsum.photos/500/500?random=808\n"
     ]
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "Polars",
     "sheet_delimiter": true,
     "type": "MD"
    }
   },
   "source": [
    "# Polars"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "4Qw2whLlL6iUd3CFWAuVbK",
     "type": "MD"
    }
   },
   "source": [
    "# Kni≈ænica Polars"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "Apache Spark",
     "sheet_delimiter": true,
     "type": "MD"
    }
   },
   "source": [
    "# Apache Spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "F5dpb4g06HPUi9VFBfEC7W",
     "type": "MD"
    }
   },
   "source": [
    "# üìä Apache Spark?\n",
    "- Najpopul√°rnej≈°ia Python kni≈ænica na vytv√°ranie dynamick√Ωch grafov\n",
    "- Tvorba interakt√≠vnych grafov a vizualiz√°ci√≠\n",
    "- Vhodn√° pre v√Ωvoj√°rov, d√°tov√Ωch vedcov a analytikov\n",
    "- Stƒ∫pcov√© grafy, ƒçiarov√© grafy, teplotn√© mapy, 3D grafy, geografick√© mapy \n",
    "- Pou≈æ√≠vatelia m√¥≈æu klika≈•, pribli≈æova≈• a sk√∫ma≈• √∫daje priamo v grafickom v√Ωstupe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "e0fusHyEzbNG0FXGDgQKlw",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /opt/python/envs/default/lib/python3.8/site-packages (3.4.0)\r\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/python/envs/default/lib/python3.8/site-packages (from pyspark) (0.10.9.7)\r\n",
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "kLKLEprsapRJeabYvMFLIt",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verzia pyspark: 3.4.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "t3fveMI8Une52QzWWSULGS",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pyspark' from '/opt/python/envs/default/lib/python3.8/site-packages/pyspark/__init__.py'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "MXyMo8jpp1UDlvymalrEkn",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+\n",
      "| Meno|Vek|     Mesto|Nakupy|\n",
      "+-----+---+----------+------+\n",
      "|  J√°n| 25|Bratislava| 150.5|\n",
      "| Anna| 30|    Ko≈°ice|200.75|\n",
      "|Peter| 22|    ≈Ωilina| 180.0|\n",
      "|  Eva| 35|     Nitra| 220.3|\n",
      "|Marek| 28|    Pre≈°ov| 195.9|\n",
      "+-----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "6HeEn0AEub17qfYJihHxd6",
     "type": "MD"
    }
   },
   "source": [
    "# 1. Naƒç√≠tanie a spracovanie veƒæk√Ωch d√°tov√Ωch s√∫borov\n",
    "Apache Spark je navrhnut√Ω na spracovanie veƒæk√Ωch d√°tov√Ωch s√∫borov, ktor√© sa nedaj√∫ jednoducho naƒç√≠ta≈• do pam√§te. Napr√≠klad m√¥≈æete naƒç√≠ta≈• veƒæk√Ω CSV alebo JSON s√∫bor a vykon√°va≈• agreg√°cie, anal√Ωzy a transform√°cie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "dyeQt2rrdUpiG2GO5rbRI5",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "9sAQxlC8HEII3upd4WpAAz",
     "type": "MD"
    }
   },
   "source": [
    "# 2. Pr√°ca s d√°tov√Ωmi r√°mcami (DataFrames) a SQL dopyty\n",
    "Spark m√° podporu pre SQL-like dopyty cez Spark SQL. M√¥≈æete pou≈æi≈• SQL dopyty na veƒæk√© mno≈æstvo d√°t priamo v r√°mci svojho d√°tov√©ho r√°mca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "wVStSkYnbcMSXUpb3fvefd",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "datalore": {
   "base_environment": "default",
   "computation_mode": "JUPYTER",
   "package_manager": "pip",
   "packages": [],
   "report_row_ids": [
    "BuaxZHVowM1NDgIm5SGTW6",
    "nT01gqoCPDuxVxR1lZEjaG",
    "HnEJbqSEA5YeuN0S9J1MeG",
    "dt5hnZzXUFlltVlRnzQ6cE",
    "iF77wHN3KjirO3UBNMvq29",
    "fkCSJJhXTqtJ0foV4CZkjt",
    "E9cX2hSUAeBjar0TwM5syB",
    "csTaUBzwY1G6f6IoNz36lu",
    "WBOAojo2SfYR7FtzUsMDna",
    "4Ml2nJBg3YA5b888QcDtvX",
    "OUKhS7mp8fwrY3964Ez5y2",
    "m2yJ0WSWZS9uU0PymTdk79",
    "g0B6TRajuZsVSDETVtF4XR",
    "dLyUYYqxeb0pGMnsTKYOJk",
    "XmrrntkpfqpOEv14b1AWu4",
    "xdvYR1r5V3JHEb1AKLtndL",
    "2Bx7dAyMwfFgln8QkHrpJq",
    "YLZVSgnIbSmavWbowrdS8q",
    "g7KfLAHAMyhSnO4Na3rtV2",
    "xS8HaGjyVIprXoHLzaZeG6",
    "yDnUZRdNbM8mTn9X47mDZP",
    "DDn7df11BmiRIZOvOfmH3x",
    "S0Db82KeEH6AMyZTsAzoNq",
    "OtQfIcnqT3bqgAGqS1GBrC",
    "ODTjgljc4F3gclXmhwTVSu",
    "U8WIV4tF0QEKHBplKZyDbc",
    "9nUi8dnJaJs1hA8dxPH3Hl",
    "PFqY3pQMHxzDThg6JKK739",
    "FMUGO3BhTNVzCt2y6deLRi",
    "RgqbOcIgNklcFGM46c39fN",
    "6V665QJ9YOu9wzx7OCKIhb",
    "myhhnezgQ0zvYlWpblthei",
    "y4aNfeuepxv0VtxamAA5vR",
    "micetiPPogLZ8yu6ioRsxF",
    "jBbIE1LidcCwStSAPH686f",
    "FHLb1t5D4UzHBPFKU8z9be",
    "HILTdb7mG4v80s6SHesxY7",
    "piFepyW9JQBjGesRNWdPHa",
    "X6nqi3oJwt16DJftTl0Z0q",
    "05Cu7Nw9CpGsa9aW93eKFA",
    "CJU41iHhNU085OV3ueY0PG",
    "OOGPy5OMyaVQSFQgwsNSSK",
    "CjgWXCSVrTczqlFotxVOXu",
    "djVznlnjdEwp9DpWqJCd58",
    "XNnoBuYVjp4XptuOzsnllh",
    "SZ4tB7Mrwqg6y4Ep90mk03",
    "SEP0IYSTXURbeKKiUMzukE",
    "mbdbzZklzOhFA9VaNO2LuF",
    "L7c9OdwRdJCrVIJjBI9FTE",
    "rruqSWkjmMnZ2BIvB3MSgX",
    "titqCmfkhMtEm1p8NjaGqC",
    "p9DgjhAmfNTcIvw8PlvEvn",
    "1AGXVeheUwlx0CoYOKaVos",
    "qTSjJR9z2qsSoz8xBd2Ixz",
    "MiCZDHRulYDa8k83MEERnW",
    "q6WzQrLyYicFIz3mhHX9rh",
    "uCjThPLnrYKsUJWnC8Rdbc",
    "0YQg2poAuoghVGOMNpoiNy",
    "G7EWHNHgoSNB0QBANJfxs3",
    "S8iYHru66vQ4dKnsKHWIqm",
    "5kv1w4O5BsIKftFfvTYt9R",
    "QVcZYHEOXcko9BWCtk55bz",
    "aJP3y2yXBzjJFE1t5NJMIN",
    "JW6knZSqV20BbILMOudfR1",
    "PCzzRbx4j9O16J55u6XezX",
    "lIsaieeolgRO4Vobl2iJSN",
    "aTu6IdH6uXEwUIlBHYhTcy"
   ],
   "version": 3
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
