{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "Knižnice HPC",
     "sheet_delimiter": true,
     "type": "MD"
    }
   },
   "source": [
    "# Knižnice HPC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "gtjaH8gOiCJE2ArnONNN4I",
     "report_properties": {
      "rowId": "BuaxZHVowM1NDgIm5SGTW6"
     },
     "type": "MD"
    }
   },
   "source": [
    "## 🐍 Kurz Vysokovýkonné počítanie a veľké dáta v Pythone (HPC)\n",
    "**🧑‍🏫 Lektor:** Miroslav Reiter  \n",
    "**📥 LinkedIn kontakt:** https://www.linkedin.com/in/miroslav-reiter/  \n",
    "\n",
    "`*Testovaci* **Text**`\n",
    "\n",
    "**✅ Osnova:** https://itkurzy.sav.sk/node/231\n",
    "\n",
    "**🎞️ YouTube videá:** https://www.youtube.com/c/IT-AcademySK  \n",
    "**📇 Zdrojové kódy a materiály:** https://github.com/miroslav-reiter/Kurzy_SAV_Analytika_Python_R  \n",
    "\n",
    "**😊 Emojis:** Win + .  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "HlJXCNvBPNinkUIhkHUYU9",
     "report_properties": {
      "rowId": "nT01gqoCPDuxVxR1lZEjaG"
     },
     "type": "MD"
    }
   },
   "source": [
    "# ⚒️ Používané nástroje HPC\n",
    "- 🐍 **Python**: momentálne najpopulárnejší jazyk pre vedu o údajoch (Data science). Python je síce interpretovaný a pomalší jazyk ako C/C++, ale využitím knižníc, ako je NumPy (a Pandas navyše), sa môžeme priblížiť rýchlosti C.  \n",
    "  \n",
    "- 🪐 **Jupyter**: nástroj s prostredím interaktívneho notebooku, mocný prieskumný, vizualizačný a komunikačný nástroj.\n",
    "  \n",
    "- 🐼 **Pandas**: nástroj na manipuláciu s tabuľkovými údajmi v Pythone. Je to obrovská knižnica, ale ak poznáte správne metódy, môžeme vytvoriť výkonnú analýzu.\n",
    "\n",
    "- 🦾 **Polars**: rýchla knižnica na spracovanie dát v Pythone, ktorá je alternatívou k Pandas. Polars je optimalizovaná pre výkon, podporuje paralelné spracovanie dát a je schopná spracovať obrovské množstvá dát oveľa rýchlejšie ako Pandas.\n",
    "\n",
    "- 📦 **Parquet**: Parquet je stĺpcový formát na ukladanie dát, ktorý je optimalizovaný pre čítanie a zápis veľkých datasetov. Je bežne používaný v big data aplikáciách a podporuje efektívne kompresie a rýchle vyhľadávanie.\n",
    "\n",
    "- 🔥 **Apache Spark**: open-source distribuovaná výpočtová platforma, ktorá umožňuje paralelné spracovanie veľkých objemov dát. Je široko používaná na analýzu big data, strojové učenie a spracovanie dát v cloude. Spark poskytuje podporu pre spracovanie dát v reálnom čase a je kompatibilný s formátmi ako Parquet a CSV.\n",
    "  \n",
    "- 📊 **NumPy**: pre numerické výpočty v Pythone, ktorá poskytuje efektívnu prácu s viacrozmernými poľami a maticami. Používa sa na rýchle numerické operácie, lineárnu algebru a spracovanie veľkých dát.\n",
    "\n",
    "- 🔬 **SciPy**: postavená na NumPy a pridáva pokročilé matematické funkcie ako numerická integrácia, optimalizácia a štatistika. Je ideálna pre vedecké a inžinierske výpočty.\n",
    "\n",
    "- 🖧 **Dask**: paralelné a distribuované výpočty na veľkých dátach cez viaceré procesy a uzly. Je kompatibilný s Pandas a NumPy, čím zjednodušuje prácu s veľkými datasetmi v distribuovaných prostrediach.\n",
    "\n",
    "- 🤖 **PyTorch**: pre strojové učenie, ktorá podporuje výpočty na GPU. Je obľúbená pre dynamické modelovanie a experimentovanie, hlavne v oblasti počítačového videnia a NLP.\n",
    "\n",
    "- 📈 **TensorFlow**: na vytváranie a nasadzovanie modelov strojového učenia v produkčných prostrediach. Je optimalizovaná na vysoký výkon, distribúciu a spracovanie veľkých dát.\n",
    "\n",
    "- 🔗 **MPI for Python (mpi4py)**: na implementáciu paralelných algoritmov cez MPI (Message Passing Interface), určená pre distribuované a paralelné výpočty v prostrediach ako superpočítače a klastry.\n",
    "\n",
    "- ⚡ **CUDA**: platforma a API od NVIDIA na paralelné výpočty pomocou GPU. PyCUDA umožňuje využívať GPU pre vysokovýkonné výpočty, čo urýchľuje aplikácie ako simulácie, spracovanie obrazu a strojové učenie.\n",
    "\n",
    "😎 Existuje 300 000+ knižníc pre Python.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "utjXvhnaVEZx0y9tcUL1AW",
     "report_properties": {
      "rowId": "HnEJbqSEA5YeuN0S9J1MeG"
     },
     "type": "MD"
    }
   },
   "source": [
    "# 📚 PIP, Knižnice a ich verzie \n",
    "**PIP (package installer for Python)** - Správca Balíčkov pre moduly programovacieho jazyka Python, Tiež napísaný v Pythone, Licencia MIT.  \n",
    "Dokumentácia PIP: https://pip.pypa.io/en/stable/\n",
    "\n",
    "**PyPI (The Python Package Index)** - Repozitár/úložisko softvéru pre programovací jazyk Python  \n",
    "https://pypi.org\n",
    "1\n",
    "\n",
    "!pip list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "Qf2fGBT29Tp85ig2h3H6be",
     "report_properties": {
      "rowId": "iF77wHN3KjirO3UBNMvq29"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package           Version\n",
      "----------------- -----------\n",
      "absl-py           2.1.0\n",
      "asttokens         2.4.1\n",
      "bokeh             3.6.1\n",
      "click             8.1.7\n",
      "cloudpickle       3.1.0\n",
      "colorama          0.4.6\n",
      "comm              0.2.2\n",
      "contourpy         1.3.1\n",
      "cycler            0.12.1\n",
      "Cython            3.0.11\n",
      "dask              2024.11.2\n",
      "dask-expr         1.1.19\n",
      "debugpy           1.8.8\n",
      "decorator         5.1.1\n",
      "distributed       2024.11.2\n",
      "executing         2.1.0\n",
      "fonttools         4.54.1\n",
      "fsspec            2024.10.0\n",
      "h5py              3.12.1\n",
      "ipykernel         6.29.5\n",
      "ipython           8.29.0\n",
      "jedi              0.19.2\n",
      "Jinja2            3.1.4\n",
      "joblib            1.4.2\n",
      "jupyter_client    8.6.3\n",
      "jupyter_core      5.7.2\n",
      "keras             3.6.0\n",
      "kiwisolver        1.4.7\n",
      "locket            1.0.0\n",
      "lz4               4.3.3\n",
      "markdown-it-py    3.0.0\n",
      "MarkupSafe        3.0.2\n",
      "matplotlib        3.9.2\n",
      "matplotlib-inline 0.1.7\n",
      "mdurl             0.1.2\n",
      "ml_dtypes         0.5.0\n",
      "mpi4py            4.0.1\n",
      "msgpack           1.1.0\n",
      "namex             0.0.8\n",
      "nest-asyncio      1.6.0\n",
      "numpy             2.1.3\n",
      "optree            0.13.1\n",
      "packaging         24.2\n",
      "pandas            2.2.3\n",
      "parquet           1.3.1\n",
      "parso             0.8.4\n",
      "partd             1.4.2\n",
      "pillow            11.0.0\n",
      "pip               24.3.1\n",
      "platformdirs      4.3.6\n",
      "plotly            5.24.1\n",
      "ply               3.11\n",
      "polars            1.13.0\n",
      "prompt_toolkit    3.0.48\n",
      "psutil            6.1.0\n",
      "pure_eval         0.2.3\n",
      "py4j              0.10.9.7\n",
      "pyarrow           18.0.0\n",
      "Pygments          2.18.0\n",
      "pyparsing         3.2.0\n",
      "pyspark           3.5.3\n",
      "python-dateutil   2.9.0.post0\n",
      "pytube            15.0.0\n",
      "pytz              2024.2\n",
      "pywin32           308\n",
      "PyYAML            6.0.2\n",
      "pyzmq             26.2.0\n",
      "rich              13.9.4\n",
      "scikit-learn      1.5.2\n",
      "scipy             1.14.1\n",
      "seaborn           0.13.2\n",
      "six               1.16.0\n",
      "sortedcontainers  2.4.0\n",
      "squarify          0.4.4\n",
      "stack-data        0.6.3\n",
      "tblib             3.0.0\n",
      "tenacity          9.0.0\n",
      "threadpoolctl     3.5.0\n",
      "thriftpy2         0.5.2\n",
      "toolz             1.0.0\n",
      "tornado           6.4.1\n",
      "traitlets         5.14.3\n",
      "typing_extensions 4.12.2\n",
      "tzdata            2024.2\n",
      "urllib3           2.2.3\n",
      "wcwidth           0.2.13\n",
      "xyzservices       2024.9.0\n",
      "zict              3.0.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: parquet in c:\\users\\bulko\\appdata\\roaming\\python\\python313\\site-packages (1.3.1)\n",
      "Requirement already satisfied: thriftpy2 in c:\\users\\bulko\\appdata\\roaming\\python\\python313\\site-packages (from parquet) (0.5.2)\n",
      "Requirement already satisfied: Cython>=3.0.10 in c:\\users\\bulko\\appdata\\roaming\\python\\python313\\site-packages (from thriftpy2->parquet) (3.0.11)\n",
      "Requirement already satisfied: ply<4.0,>=3.4 in c:\\users\\bulko\\appdata\\roaming\\python\\python313\\site-packages (from thriftpy2->parquet) (3.11)\n",
      "Requirement already satisfied: six~=1.15 in c:\\users\\bulko\\appdata\\roaming\\python\\python313\\site-packages (from thriftpy2->parquet) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip list\n",
    "\n",
    "#!pip install numpy\n",
    "#!pip install scipy\n",
    "#!pip install dask[complete]\n",
    "#!pip install torch\n",
    "#!pip install tensorflow\n",
    "#!pip install polars\n",
    "!pip install parquet\n",
    "#!pip install pyspark\n",
    "#!pip install pyarrow\n",
    "#!pip install mpi4py\n",
    "#!pip install pycuda"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "O6f5kWGeujRXtOQVWAlLTl",
     "report_properties": {
      "rowId": "E9cX2hSUAeBjar0TwM5syB"
     },
     "type": "MD"
    }
   },
   "source": [
    "# 📜 Verzie vizualizačných knižnic \n",
    "try - except - finally  \n",
    "\n",
    "https://docs.python.org/3/tutorial/errors.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "V0v8HOOhd9hhVxJnX8q4eL",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Moduly/kniznice boli nacitane\n",
      "Pandas verzia 2.2.3\n",
      "Numpy verzia 2.1.3\n",
      "Scipy verzia 1.14.1\n",
      "Dask verzia 2024.11.2\n",
      "Polars verzia 1.13.0\n",
      "PyArrow verzia 18.0.0\n",
      "Parquet verzia 1.3.1\n",
      "Pyspark verzia 3.5.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import scipy \n",
    "import dask\n",
    "#import torch\n",
    "#import tensorflow as tf\n",
    "\n",
    "import pyspark\n",
    "# na pracu s parquet format\n",
    "import pyarrow\n",
    "import parquet\n",
    "\n",
    "import importlib.metadata\n",
    "\n",
    "print(\"✅ Moduly/kniznice boli nacitane\")\n",
    "# Zobrazenie verzií knižníc\n",
    "print(f\"Pandas verzia {pd.__version__}\")\n",
    "print(f\"Numpy verzia {np.__version__}\")\n",
    "print(f\"Scipy verzia {scipy.__version__}\")\n",
    "print(f\"Dask verzia {dask.__version__}\")\n",
    "#print(f\"Pytorch verzia {torch.__version__}\")\n",
    "#print(f\"Tensorflow verzia {tf.__version__}\")\n",
    "\n",
    "print(f\"Polars verzia {pl.__version__}\")\n",
    "\n",
    "print(f\"PyArrow verzia {pyarrow.__version__}\")\n",
    "print(f\"Parquet verzia {importlib.metadata.version('parquet')}\")\n",
    "\n",
    "print(f\"Pyspark verzia {pyspark.__version__}\")\n",
    "\n",
    "\n",
    "\n",
    "# Tu používame pyarrow pre Parquet\n",
    " \n",
    "\n",
    "# Ak by bol nainštalovaný `pyarrow` pre Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "xxC61eI2Oqyzy8GmzzwrET",
     "report_properties": {
      "rowId": "csTaUBzwY1G6f6IoNz36lu"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Moduly/kniznice boli nacitane\n",
      "Pandas verzia 2.2.3\n",
      "Numpy verzia 2.1.3\n",
      "Scipy verzia 1.14.1\n",
      "Dask verzia 2024.11.2\n",
      "Polars verzia 1.13.0\n",
      "PyArrow verzia 18.0.0\n",
      "Parquet verzia 1.3.1\n",
      "Pyspark verzia 3.5.3\n",
      "\n",
      "FINALLY: Tento text sa vypise vzdy\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import pandas as pd\n",
    "    import polars as pl\n",
    "    import numpy as np\n",
    "    import scipy \n",
    "    import dask\n",
    "    #import torch\n",
    "    #import tensorflow as tf\n",
    "\n",
    "    import pyspark\n",
    "# na pracu s parquet format\n",
    "    import pyarrow\n",
    "    import parquet\n",
    "\n",
    "    import importlib.metadata\n",
    "\n",
    "    print(\"✅ Moduly/kniznice boli nacitane\")\n",
    "# Zobrazenie verzií knižníc\n",
    "    print(f\"Pandas verzia {pd.__version__}\")\n",
    "    print(f\"Numpy verzia {np.__version__}\")\n",
    "    print(f\"Scipy verzia {scipy.__version__}\")\n",
    "    print(f\"Dask verzia {dask.__version__}\")\n",
    "    #print(f\"Pytorch verzia {torch.__version__}\")\n",
    "    #print(f\"Tensorflow verzia {tf.__version__}\")\n",
    "\n",
    "    print(f\"Polars verzia {pl.__version__}\")\n",
    "\n",
    "    print(f\"PyArrow verzia {pyarrow.__version__}\")\n",
    "    print(f\"Parquet verzia {importlib.metadata.version('parquet')}\")\n",
    "\n",
    "    print(f\"Pyspark verzia {pyspark.__version__}\")\n",
    "\n",
    "\n",
    "\n",
    "# Tu používame pyarrow pre Parquet\n",
    " \n",
    "\n",
    "# Ak by bol nainštalovaný `pyarrow` pre Parquet\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "except:\n",
    "    print(\"EXCEPT: Moduly/Kniznice neboli nacitane, nastala chyba...\")\n",
    "\n",
    "finally:\n",
    "    print(\"\\nFINALLY: Tento text sa vypise vzdy\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "9nZeU67HPQXJSgN5qZb7h6",
     "type": "MD"
    }
   },
   "source": [
    "# 🤖 TensorFlow: Základný model neurónovej siete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "ZcvVIYsnByU77rGQw9mYQw",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential([\n\u001b[0;32m      6\u001b[0m     Dense(\u001b[38;5;241m32\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m784\u001b[39m,)),\n\u001b[0;32m      7\u001b[0m     Dense(\u001b[38;5;241m10\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m ])\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, activation=\"relu\", input_shape=(784,)),\n",
    "    Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer = \"adam\",\n",
    "    loss = \"sparse_categorical_crossentropy\",\n",
    "    metrics = [\"accuracy\"]\n",
    ")\n",
    "\n",
    "X_train = np.random.random((1000, 784))\n",
    "Y_train = np.random.randint(10, size = (1000, ))\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    epochs = 5\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "tsdIQ9XGjereK4lpOTMNED",
     "type": "MD"
    }
   },
   "source": [
    "# ⚡ Polars: Rýchle spracovanie dátových rámcov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "lxu23ZZDtd75p5JMDyXnt8",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (4, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>A</th><th>B</th><th>C</th></tr><tr><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>1</td><td>5</td><td>1</td></tr><tr><td>2</td><td>6</td><td>2</td></tr><tr><td>3</td><td>7</td><td>3</td></tr><tr><td>4</td><td>8</td><td>4</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (4, 3)\n",
       "┌─────┬─────┬─────┐\n",
       "│ A   ┆ B   ┆ C   │\n",
       "│ --- ┆ --- ┆ --- │\n",
       "│ i64 ┆ i64 ┆ i64 │\n",
       "╞═════╪═════╪═════╡\n",
       "│ 1   ┆ 5   ┆ 1   │\n",
       "│ 2   ┆ 6   ┆ 2   │\n",
       "│ 3   ┆ 7   ┆ 3   │\n",
       "│ 4   ┆ 8   ┆ 4   │\n",
       "└─────┴─────┴─────┘"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "df = pl.DataFrame({\n",
    "    \"A\": [1,2,3,4],\n",
    "    \"B\": [5,6,7,8]\n",
    "\n",
    "}\n",
    ")\n",
    "\n",
    "#print(df)\n",
    "#df\n",
    "\n",
    "df_filtrovany = df.filter(pl.col(\"A\")>2)\n",
    "#df_filtrovany\n",
    "\n",
    "df2 = df.with_columns(pl.col(\"A\").alias(\"C\"))\n",
    "df2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "B6sjX6sDMFTUWo9sNL2dyT",
     "type": "MD"
    }
   },
   "source": [
    "# 📦 PyArrow: Práca s Parquet formátom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "Go4cn1W4BSpOvJIdPZK4vf",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "meno: string\n",
       "vek: int32\n",
       "povolania: string\n",
       "----\n",
       "meno: [[\"Dada\",\"Martin\",\"Michal\"]]\n",
       "vek: [[21,28,29]]\n",
       "povolania: [[\"Inžinier\",\"Doktor\",\"Umelec\"]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq \n",
    "import pandas as pd\n",
    "\n",
    "data = [\n",
    "    (\"Dada\", 21, \"Inžinier\"),\n",
    "    (\"Martin\", 28, \"Doktor\"),\n",
    "    (\"Michal\", 29, \"Umelec\")\n",
    "]\n",
    "\n",
    "schema = pa.schema([\n",
    "    (\"meno\", pa.string()),\n",
    "    (\"vek\", pa.int32()),\n",
    "    (\"povolania\", pa.string())\n",
    "])\n",
    "\n",
    "tabulka = pa.Table.from_pandas(pd.DataFrame(data, columns=[\"meno\", \"vek\", \"povolania\"]), schema = schema)\n",
    "\n",
    "pq.write_table(tabulka, \"zamestnanci.parquet\")\n",
    "\n",
    "\n",
    "tabulka_nacitana = pq.read_table(\"zamestnanci.parquet\")\n",
    "tabulka_nacitana"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "H6NTYiRAnYfsGkZA9mtMGr",
     "type": "MD"
    }
   },
   "source": [
    "# 🔥 PySpark: Práca s distribuovanými dátami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "Zad9yKX9TuNDrxSMn2AyFX",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.UnsupportedOperationException: getSubject is supported only if a security manager is allowed\r\n\tat java.base/javax.security.auth.Subject.getSubject(Subject.java:347)\r\n\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:577)\r\n\tat org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2416)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2416)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:329)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:501)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:485)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1575)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m----> 3\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPrikladPySpark\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m data \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      6\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRobert\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m40\u001b[39m),\n\u001b[0;32m      7\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJan\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m30\u001b[39m),\n\u001b[0;32m      8\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMartin\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m      9\u001b[0m ]\n\u001b[0;32m     11\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(data, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeno\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvek\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pyspark\\sql\\session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pyspark\\context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pyspark\\context.py:203\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    201\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mudf_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pyspark\\context.py:296\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    295\u001b[0m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[1;32m--> 296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;241m=\u001b[39m jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m SparkConf(_jconf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39mconf())\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pyspark\\context.py:421\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[1;34m(self, jconf)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;124;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\py4j\\java_gateway.py:1587\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1581\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1582\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1583\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1584\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1586\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1587\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1588\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1590\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1591\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.UnsupportedOperationException: getSubject is supported only if a security manager is allowed\r\n\tat java.base/javax.security.auth.Subject.getSubject(Subject.java:347)\r\n\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:577)\r\n\tat org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2416)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2416)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:329)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:501)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:485)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1575)\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PrikladPySpark\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (\"Robert\", 40),\n",
    "    (\"Jan\", 30),\n",
    "    (\"Martin\", 20)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"meno\", \"vek\"])\n",
    "df.show()\n",
    "\n",
    "df_filtrovany = df.filter(df[\"vek\"] >= 30)\n",
    "df_filtrovany.show()\n",
    "\n",
    "df.write.mode(\"ignore\").parquet(\"studenti.parquet\")\n",
    "\n",
    "df_nacitany = spark.read.parquet(\"studenti.parquet\")\n",
    "df_nacitany.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "Testovacie Dáta a Generovanie Dát",
     "sheet_delimiter": true,
     "type": "MD"
    }
   },
   "source": [
    "# Testovacie Dáta a Generovanie Dát"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "HcTDYNQOTjjFJgdsyA5vLI",
     "type": "MD"
    }
   },
   "source": [
    "# Testovacie Dáta a Generovanie Dát"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "iFZ3t2RO2PxxNPuUGpVNXI",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "zaznamy = 8_000_000\n",
    "\n",
    "data_klienti = {\n",
    "    \"Meno\": np.random.choice([\n",
    "        \"Adriana\", \"Alena\", \"Andrea\", \"Anna\", \"Barbora\", \"Beáta\", \"Daniela\", \"Dominika\", \"Elena\", \"Emília\", \"Erika\", \"Eva\", \"Gabriela\", \"Hana\", \"Ivana\", \"Jana\", \"Katarína\", \"Kristína\", \n",
    "    \"Laura\", \"Lenka\", \"Lucia\", \"Magdaléna\", \"Mária\", \"Martina\", \"Monika\", \"Natália\", \"Petra\", \"Renáta\", \"Silvia\", \"Simona\", \"Soňa\", \"Tamara\", \"Veronika\", \"Zuzana\"\n",
    "    ], size = zaznamy),\n",
    "    \"Vek\": np.random.randint(18, 71, size = zaznamy),\n",
    "    \"Mesta\": np.random.choice([\"Bratislava\", \"Košice\", \"Prešov\", \"Žilina\", \"Nitra\", \"Banská Bystrica\", \"Trnava\", \"Martin\", \"Trenčín\", \"Poprad\"], size = zaznamy),\n",
    "    \"Nakupy\": np.round(np.random.uniform(150.0, 1000.0, size = zaznamy), 2)\n",
    "}\n",
    "\n",
    "df_data_klienti = pd.DataFrame(data_klienti)\n",
    "df_data_klienti\n",
    "df_data_klienti.to_csv(\"dataset-8M-klient-sk.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "IUSdvy3AyzABVb5YhVEQZ7",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Počet záznamov\n",
    "zaznamy = 1_000_000_000\n",
    "# Veľkosť jednej časti\n",
    "velkost_casti = 10_000_000\n",
    "\n",
    "# Cesta k súboru\n",
    "csv_soubor = \"dataset-1000M-klienti-SK.csv\"\n",
    "\n",
    "# Nastavíme, aby sa súbor prepisoval od začiatku, keďže budeme zapisovať po častiach\n",
    "with open(csv_soubor, mode=\"a\") as f:\n",
    "    # Vytvorenie dát a zápis po častiach\n",
    "    for i in range(zaznamy // velkost_casti):\n",
    "        data_klienti = {\n",
    "            \"Meno\": np.random.choice([\n",
    "                \"Adriana\", \"Alena\", \"Andrea\", \"Anna\", \"Barbora\", \"Beáta\", \"Daniela\", \"Dominika\", \n",
    "                \"Elena\", \"Emília\", \"Erika\", \"Eva\", \"Gabriela\", \"Hana\", \"Ivana\", \"Jana\", \"Katarína\", \n",
    "                \"Kristína\", \"Laura\", \"Lenka\", \"Lucia\", \"Magdaléna\", \"Mária\", \"Martina\", \"Monika\", \n",
    "                \"Natália\", \"Petra\", \"Renáta\", \"Silvia\", \"Simona\", \"Soňa\", \"Tamara\", \"Veronika\", \"Zuzana\"], \n",
    "                size=velkost_casti\n",
    "            ),\n",
    "            \"Vek\": np.random.randint(18, 70, size=velkost_casti),\n",
    "            \"Mesto\": np.random.choice([\n",
    "                \"Bratislava\", \"Košice\", \"Prešov\", \"Žilina\", \"Nitra\", \n",
    "                \"Banská Bystrica\", \"Trnava\", \"Martin\", \"Trenčín\", \"Poprad\"\n",
    "            ], size=velkost_casti),\n",
    "            \"Nakupy\": np.round(np.random.uniform(150.0, 1000.0, size=velkost_casti), 2)\n",
    "        }\n",
    "        \n",
    "        # Vytvorenie DataFrame pre aktuálnu časť\n",
    "        df_data_klienti = pd.DataFrame(data_klienti)\n",
    "        \n",
    "        # Prvýkrát zapisujeme hlavičku, ostatné časti už nie\n",
    "        df_data_klienti.to_csv(csv_soubor, mode=\"a\", header=(i == 0), index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "jlFOtTvW4oV7pW67aR0SQe",
     "type": "MD"
    }
   },
   "source": [
    "# 📚 Knižnica Faker\n",
    "- Populárna knižnica v Pythone, ktorá slúži na generovanie falošných údajov, ako sú mená, adresy, e-maily, telefónne čísla a ďalšie. \n",
    "- Je veľmi užitočná pri testovaní aplikácií, generovaní testovacích dát alebo pri simulácii reálnych používateľov. \n",
    "- Môžete rýchlo a jednoducho vytvárať realisticky vyzerajúce, ale úplne náhodné údaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "YkTIwPISvzAI8KhHgAYDSC",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faker\r\n",
      "  Downloading Faker-30.8.2-py3-none-any.whl.metadata (15 kB)\r\n",
      "Requirement already satisfied: python-dateutil>=2.4 in /opt/python/envs/default/lib/python3.8/site-packages (from faker) (2.9.0.post0)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/python/envs/default/lib/python3.8/site-packages (from faker) (4.5.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/python/envs/default/lib/python3.8/site-packages (from python-dateutil>=2.4->faker) (1.16.0)\r\n",
      "Downloading Faker-30.8.2-py3-none-any.whl (1.8 MB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m106.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: faker\r\n",
      "Successfully installed faker-30.8.2\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "B67OvR2oyWzIqMQH5N0XJt",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. používateľ : Václav Mráz\n",
      "Email: václav.mráz1@gmail.com\n",
      "Adresa: Nová Bellova 3\n",
      "804 04 Licince\n",
      "Telefón: 00421 948 582 742\n",
      "----------------------------------------\n",
      "2. používateľ : Kazimír Klimková\n",
      "Email: kazimír.klimková2@gmail.com\n",
      "Adresa: Malý trh 66\n",
      "828 54 Nevidzany\n",
      "Telefón: 00421 948 545 248\n",
      "----------------------------------------\n",
      "3. používateľ : Lolita Dorová\n",
      "Email: lolita.dorová3@gmail.com\n",
      "Adresa: Špieszova 7\n",
      "960 45 Malé Kršteňany\n",
      "Telefón: 00421 58 612 2829\n",
      "----------------------------------------\n",
      "4. používateľ : Miriama Jendeková\n",
      "Email: miriama.jendeková4@gmail.com\n",
      "Adresa: Údolná 82\n",
      "996 85 Skalka nad Váhom\n",
      "Telefón: +421 949 665 893\n",
      "----------------------------------------\n",
      "5. používateľ : Linda Pavelková\n",
      "Email: linda.pavelková5@gmail.com\n",
      "Adresa: Pasienková 3\n",
      "042 23 Stanča\n",
      "Telefón: 00421 901 245 494\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "vrn96tXTQ6yqoIhkl19ZqK",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "generuj_obrazok() takes 0 positional arguments but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------",
      "Traceback (most recent call last)",
      "generuj_obrazok() takes 0 positional arguments but 2 were given"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "p4tLTia16lRqDEUC3Bjvq7",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL falošného obrázka: https://picsum.photos/500/500?random=808\n"
     ]
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "Polars",
     "sheet_delimiter": true,
     "type": "MD"
    }
   },
   "source": [
    "# Polars"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "4Qw2whLlL6iUd3CFWAuVbK",
     "type": "MD"
    }
   },
   "source": [
    "# Knižnica Polars"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "Apache Spark",
     "sheet_delimiter": true,
     "type": "MD"
    }
   },
   "source": [
    "# Apache Spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "F5dpb4g06HPUi9VFBfEC7W",
     "type": "MD"
    }
   },
   "source": [
    "# 📊 Apache Spark?\n",
    "- Najpopulárnejšia Python knižnica na vytváranie dynamických grafov\n",
    "- Tvorba interaktívnych grafov a vizualizácií\n",
    "- Vhodná pre vývojárov, dátových vedcov a analytikov\n",
    "- Stĺpcové grafy, čiarové grafy, teplotné mapy, 3D grafy, geografické mapy \n",
    "- Používatelia môžu klikať, približovať a skúmať údaje priamo v grafickom výstupe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "e0fusHyEzbNG0FXGDgQKlw",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /opt/python/envs/default/lib/python3.8/site-packages (3.4.0)\r\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/python/envs/default/lib/python3.8/site-packages (from pyspark) (0.10.9.7)\r\n",
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "kLKLEprsapRJeabYvMFLIt",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verzia pyspark: 3.4.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "t3fveMI8Une52QzWWSULGS",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pyspark' from '/opt/python/envs/default/lib/python3.8/site-packages/pyspark/__init__.py'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "MXyMo8jpp1UDlvymalrEkn",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+\n",
      "| Meno|Vek|     Mesto|Nakupy|\n",
      "+-----+---+----------+------+\n",
      "|  Ján| 25|Bratislava| 150.5|\n",
      "| Anna| 30|    Košice|200.75|\n",
      "|Peter| 22|    Žilina| 180.0|\n",
      "|  Eva| 35|     Nitra| 220.3|\n",
      "|Marek| 28|    Prešov| 195.9|\n",
      "+-----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "6HeEn0AEub17qfYJihHxd6",
     "type": "MD"
    }
   },
   "source": [
    "# 1. Načítanie a spracovanie veľkých dátových súborov\n",
    "Apache Spark je navrhnutý na spracovanie veľkých dátových súborov, ktoré sa nedajú jednoducho načítať do pamäte. Napríklad môžete načítať veľký CSV alebo JSON súbor a vykonávať agregácie, analýzy a transformácie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "dyeQt2rrdUpiG2GO5rbRI5",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "9sAQxlC8HEII3upd4WpAAz",
     "type": "MD"
    }
   },
   "source": [
    "# 2. Práca s dátovými rámcami (DataFrames) a SQL dopyty\n",
    "Spark má podporu pre SQL-like dopyty cez Spark SQL. Môžete použiť SQL dopyty na veľké množstvo dát priamo v rámci svojho dátového rámca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "wVStSkYnbcMSXUpb3fvefd",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "datalore": {
   "base_environment": "default",
   "computation_mode": "JUPYTER",
   "package_manager": "pip",
   "packages": [],
   "report_row_ids": [
    "BuaxZHVowM1NDgIm5SGTW6",
    "nT01gqoCPDuxVxR1lZEjaG",
    "HnEJbqSEA5YeuN0S9J1MeG",
    "dt5hnZzXUFlltVlRnzQ6cE",
    "iF77wHN3KjirO3UBNMvq29",
    "fkCSJJhXTqtJ0foV4CZkjt",
    "E9cX2hSUAeBjar0TwM5syB",
    "csTaUBzwY1G6f6IoNz36lu",
    "WBOAojo2SfYR7FtzUsMDna",
    "4Ml2nJBg3YA5b888QcDtvX",
    "OUKhS7mp8fwrY3964Ez5y2",
    "m2yJ0WSWZS9uU0PymTdk79",
    "g0B6TRajuZsVSDETVtF4XR",
    "dLyUYYqxeb0pGMnsTKYOJk",
    "XmrrntkpfqpOEv14b1AWu4",
    "xdvYR1r5V3JHEb1AKLtndL",
    "2Bx7dAyMwfFgln8QkHrpJq",
    "YLZVSgnIbSmavWbowrdS8q",
    "g7KfLAHAMyhSnO4Na3rtV2",
    "xS8HaGjyVIprXoHLzaZeG6",
    "yDnUZRdNbM8mTn9X47mDZP",
    "DDn7df11BmiRIZOvOfmH3x",
    "S0Db82KeEH6AMyZTsAzoNq",
    "OtQfIcnqT3bqgAGqS1GBrC",
    "ODTjgljc4F3gclXmhwTVSu",
    "U8WIV4tF0QEKHBplKZyDbc",
    "9nUi8dnJaJs1hA8dxPH3Hl",
    "PFqY3pQMHxzDThg6JKK739",
    "FMUGO3BhTNVzCt2y6deLRi",
    "RgqbOcIgNklcFGM46c39fN",
    "6V665QJ9YOu9wzx7OCKIhb",
    "myhhnezgQ0zvYlWpblthei",
    "y4aNfeuepxv0VtxamAA5vR",
    "micetiPPogLZ8yu6ioRsxF",
    "jBbIE1LidcCwStSAPH686f",
    "FHLb1t5D4UzHBPFKU8z9be",
    "HILTdb7mG4v80s6SHesxY7",
    "piFepyW9JQBjGesRNWdPHa",
    "X6nqi3oJwt16DJftTl0Z0q",
    "05Cu7Nw9CpGsa9aW93eKFA",
    "CJU41iHhNU085OV3ueY0PG",
    "OOGPy5OMyaVQSFQgwsNSSK",
    "CjgWXCSVrTczqlFotxVOXu",
    "djVznlnjdEwp9DpWqJCd58",
    "XNnoBuYVjp4XptuOzsnllh",
    "SZ4tB7Mrwqg6y4Ep90mk03",
    "SEP0IYSTXURbeKKiUMzukE",
    "mbdbzZklzOhFA9VaNO2LuF",
    "L7c9OdwRdJCrVIJjBI9FTE",
    "rruqSWkjmMnZ2BIvB3MSgX",
    "titqCmfkhMtEm1p8NjaGqC",
    "p9DgjhAmfNTcIvw8PlvEvn",
    "1AGXVeheUwlx0CoYOKaVos",
    "qTSjJR9z2qsSoz8xBd2Ixz",
    "MiCZDHRulYDa8k83MEERnW",
    "q6WzQrLyYicFIz3mhHX9rh",
    "uCjThPLnrYKsUJWnC8Rdbc",
    "0YQg2poAuoghVGOMNpoiNy",
    "G7EWHNHgoSNB0QBANJfxs3",
    "S8iYHru66vQ4dKnsKHWIqm",
    "5kv1w4O5BsIKftFfvTYt9R",
    "QVcZYHEOXcko9BWCtk55bz",
    "aJP3y2yXBzjJFE1t5NJMIN",
    "JW6knZSqV20BbILMOudfR1",
    "PCzzRbx4j9O16J55u6XezX",
    "lIsaieeolgRO4Vobl2iJSN",
    "aTu6IdH6uXEwUIlBHYhTcy"
   ],
   "version": 3
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
